import os
import json
import numpy as np
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
CLOUD = "aws"
REGION = "us-east-1"
SPEC = ServerlessSpec(cloud=CLOUD, region=REGION)
openai_client = OpenAI(api_key=OPENAI_API_KEY)
pc = Pinecone(api_key=PINECONE_API_KEY)

def clean_metadata(meta):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ dict –∏ —Å–ø–∏—Å–∫–∏ dict –≤ —Å—Ç—Ä–æ–∫—É, –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –≤—Å–µ–≥–¥–∞ dict!"""
    import numpy as np
    cleaned = {}
    for k, v in meta.items():
        if isinstance(v, dict):
            cleaned[k] = json.dumps(v, ensure_ascii=False)
        elif isinstance(v, list):
            # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ dict-–æ–≤ ‚Äî —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥—ã–π
            if any(isinstance(i, dict) for i in v):
                cleaned[k] = [json.dumps(i, ensure_ascii=False) if isinstance(i, dict) else i for i in v]
            else:
                cleaned[k] = [i if isinstance(i, (str, int, float, bool)) or i is None else str(i) for i in v]
        elif isinstance(v, np.generic):
            cleaned[k] = v.item()
        elif isinstance(v, np.ndarray):
            cleaned[k] = v.tolist()
        elif isinstance(v, (str, int, float, bool)) or v is None:
            cleaned[k] = v
        else:
            cleaned[k] = str(v)
    return cleaned

def build_index(posts, index_name="social-index", max_posts: int = None):
    existing = [i["name"] for i in pc.list_indexes()]
    if index_name not in existing:
        pc.create_index(name=index_name, dimension=1536, metric="cosine", spec=SPEC)
    idx = pc.Index(index_name)
    if max_posts:
        posts = posts[:max_posts]
    vecs = []
    for p in posts:
        if not p.get("text", "").strip() and not p.get("image_description", "").strip():
            continue
        embed_text = f"{p.get('text', '')} {p.get('image_description', '')}".strip()
        emb = openai_client.embeddings.create(input=embed_text, model="text-embedding-ada-002")
        meta = {k: v for k, v in p.items() if k != "image_url" and v is not None}
        meta = clean_metadata(meta)
        vecs.append({
            "id": p.get("post_id") or f"post_{len(vecs)}",
            "values": [float(x) for x in emb.data[0].embedding],
            "metadata": meta
        })
    print(f"\nüß† –ó–∞–≥—Ä—É–∂–∞–µ–º {len(vecs)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å ¬´{index_name}¬ª‚Ä¶")
    idx.upsert(vectors=vecs)
    print(f"‚úÖ –í—Å—ë –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(posts)} –ø–æ—Å—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ image_description.")
    return posts

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--input", required=True, help="processed_meta.json (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π)")
    parser.add_argument("-n", "--index", default="social-index")
    parser.add_argument("--max", type=int, default=None)
    args = parser.parse_args()
    posts = json.load(open(args.input, encoding="utf-8"))
    build_index(posts, index_name=args.index, max_posts=args.max)
